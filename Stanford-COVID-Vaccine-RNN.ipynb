{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(\"datasets/stanford-covid-vaccine/train.json\", \"r\")\n",
    "train_data_raw = train_file.read()\n",
    "train_data = list(map(lambda l: json.loads(l), list(filter(lambda l: len(l) > 0, train_data_raw.split(\"\\n\")))))\n",
    "train_file.close()\n",
    "\n",
    "test_file = open(\"datasets/stanford-covid-vaccine/test.json\", \"r\")\n",
    "test_data_raw = test_file.read()\n",
    "test_data = list(map(lambda l: json.loads(l), list(filter(lambda l: len(l) > 0, test_data_raw.split(\"\\n\")))))\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'id', 'sequence', 'structure', 'predicted_loop_type', 'signal_to_noise', 'SN_filter', 'seq_length', 'seq_scored', 'reactivity_error', 'deg_error_Mg_pH10', 'deg_error_pH10', 'deg_error_Mg_50C', 'deg_error_50C', 'reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GGAAAAGCUCUAAUAACAGGAGACUAGGACUACGUAUUUCUAGGUAACUGGAAUAACCCAUACCAGCAGUUAGAGUUCGCUCUAACAAAAGAAACAACAACAACAAC'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"sequence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.....((((((.......)))).)).((.....((..((((((....))))))..)).....))....(((((((....))))))).....................'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"structure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EEEEESSSSSSHHHHHHHSSSSBSSXSSIIIIISSIISSSSSSHHHHSSSSSSIISSIIIIISSXXXXSSSSSSSHHHHSSSSSSSEEEEEEEEEEEEEEEEEEEEE'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"predicted_loop_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('id_001f94081', 107, 107, 107)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"id\"], len(train_data[0][\"sequence\"]), len(train_data[0][\"structure\"]), len(train_data[0][\"predicted_loop_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = list(zip(train_data[0][\"sequence\"], train_data[0][\"structure\"], train_data[0][\"predicted_loop_type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('G', '.', 'E'), ('C', '.', 'E'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence[0], input_sequence[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3297, 0.7556, 2.3375, 0.3581, 0.6382)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"reactivity\"][0], train_data[0][\"deg_Mg_pH10\"][0], train_data[0][\"deg_pH10\"][0], train_data[0][\"deg_Mg_50C\"][0], train_data[0][\"deg_50C\"][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequence = list(zip(train_data[0][\"reactivity\"], train_data[0][\"deg_Mg_pH10\"], train_data[0][\"deg_pH10\"], train_data[0][\"deg_Mg_50C\"], train_data[0][\"deg_50C\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.3297, 0.7556, 2.3375, 0.3581, 0.6382),\n",
       " (0.5731, 0.6898, 1.172, 0.4254, 0.8472))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequence[0], output_sequence[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_categorical_encoder(input_attribute_index_or_key, categories, total_dimensions, start_index, end_index):\n",
    "    if not (end_index - start_index + 1 == len(categories)):\n",
    "        raise Exception(\"Mismatch between number of categories and dimensions assigned.\")\n",
    "    def encoder(data_row, data_vector):\n",
    "        if len(data_vector) != total_dimensions:\n",
    "            raise Exception(f\"Data vector is of size {len(data_vector)}, but should be of size {total_dimensions}.\")\n",
    "        \n",
    "        category_index = categories.index(data_row[input_attribute_index_or_key])\n",
    "        encoding_index = start_index + category_index\n",
    "        data_vector[encoding_index] = 1\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "def build_continuous_encoder(input_attribute_index_or_key, total_dimensions, attribute_index):\n",
    "    def encoder(data_row, data_vector):\n",
    "        if len(data_vector) != total_dimensions:\n",
    "            raise Exception(f\"Data vector is of size {len(data_vector)}, but should be of size {total_dimensions}.\")\n",
    "        data_vector[attribute_index] = float(data_row[input_attribute_index_or_key])\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "def encode_data(rows, encoders, total_dimensions):\n",
    "    encoded_rows = []\n",
    "    for row in rows:\n",
    "        encoded_row = [0] * total_dimensions\n",
    "        for encoder in encoders:\n",
    "            encoder(row, encoded_row)\n",
    "        encoded_rows.append(encoded_row)\n",
    "    \n",
    "    return encoded_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dim_input = 14\n",
    "total_dim_output = 5\n",
    "sequence_classes = [\"A\", \"G\", \"U\", \"C\"]\n",
    "structure_classes = [\"(\", \".\", \")\"]\n",
    "predicted_loop_type_classes = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n",
    "\n",
    "sequence_encoder = build_categorical_encoder(0, sequence_classes, total_dim_input, 0, 3)\n",
    "structure_encoder = build_categorical_encoder(1, structure_classes, total_dim_input, 4, 6)\n",
    "predicted_loop_encoder = build_categorical_encoder(2, predicted_loop_type_classes, total_dim_input, 7, 13)\n",
    "\n",
    "\n",
    "data_encoders = [\n",
    "    sequence_encoder,\n",
    "    structure_encoder,\n",
    "    predicted_loop_encoder\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input_sequence = encode_data(input_sequence, data_encoders, total_dim_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A', '(', 'S')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input_sequence[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(encoded_input_sequence, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([107, 14])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_data = list(filter(lambda d: d[\"SN_filter\"] == 1, train_data))\n",
    "dataset = []\n",
    "for row in filtered_train_data:\n",
    "    input_sequence = list(zip(row[\"sequence\"], row[\"structure\"], row[\"predicted_loop_type\"]))\n",
    "    target_sequence = list(zip(row[\"reactivity\"], row[\"deg_Mg_pH10\"], row[\"deg_pH10\"], row[\"deg_Mg_50C\"], row[\"deg_50C\"]))\n",
    "    input_sequence_encoded = encode_data(input_sequence, data_encoders, total_dim_input)\n",
    "    dataset.append(\n",
    "        (input_sequence_encoded,\n",
    "         target_sequence))\n",
    "\n",
    "\n",
    "testing_sequence_id_to_dataset_map = dict()\n",
    "testing_set = []\n",
    "for row in test_data:\n",
    "    input_sequence = list(zip(row[\"sequence\"], row[\"structure\"], row[\"predicted_loop_type\"]))\n",
    "    input_sequence_encoded = encode_data(input_sequence, data_encoders, total_dim_input)\n",
    "    testing_set.append(input_sequence_encoded)\n",
    "    testing_sequence_id_to_dataset_map[row[\"id\"]] = input_sequence_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)\n",
    "\n",
    "training = dataset[:int(len(dataset) * 0.8)]\n",
    "evaluation = dataset[int(len(dataset) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1271, 318, 3634)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training), len(evaluation), len(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size_train = 31\n",
    "batch_size_test = 318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(seq, length, padding=None):\n",
    "    if padding == None:\n",
    "        padding = 0\n",
    "    z = [padding] * (length - len(seq))\n",
    "    seq = seq + z\n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RNNModelRNA(nn.Module):\n",
    "    \n",
    "#     def __init__(self, input_dim=None, hidden_dim=None, num_layers=None, output_dim=None):\n",
    "#         super(RNNModelRNA, self).__init__()\n",
    "#         self.num_layers = num_layers\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.rnn = nn.RNN(input_dim, hidden_dim, num_layers)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "#     def init_hidden(self, batch_size):\n",
    "#         return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(\"cuda\")\n",
    "    \n",
    "#     def forward(self, x, lengths=None):\n",
    "#         batch_size = x.size(1)\n",
    "#         hidden = self.init_hidden(batch_size)\n",
    "#         if lengths is not None:\n",
    "#             x = nn.utils.rnn.pack_padded_sequence(x, lengths)\n",
    "#             output, hidden = self.rnn(x, hidden)\n",
    "#             output, lengths_unpacked = nn.utils.rnn.pad_packed_sequence(output)\n",
    "#         else:\n",
    "#             output, hidden = self.rnn(x, hidden)\n",
    "\n",
    "#         output = self.fc(output)\n",
    "\n",
    "#         return output\n",
    "\n",
    "    \n",
    "# class LSTMModelRNA(nn.Module):\n",
    "    \n",
    "#     def __init__(self, input_dim=None, hidden_dim=None, num_layers=None, output_dim=None, dropout=0.1):\n",
    "#         super(LSTMModelRNA, self).__init__()\n",
    "#         self.num_layers = num_layers\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, dropout=dropout, bidirectional=True)\n",
    "#         self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
    "    \n",
    "#     def init_hidden(self, batch_size):\n",
    "#         return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(\"cuda\")\n",
    "    \n",
    "#     def forward(self, x, lengths=None):\n",
    "#         batch_size = x.size(1)\n",
    "#         hidden = self.init_hidden(batch_size)\n",
    "#         if lengths is not None:\n",
    "#             x = nn.utils.rnn.pack_padded_sequence(x, lengths)\n",
    "#             output, _ = self.rnn(x)\n",
    "#             output, lengths_unpacked = nn.utils.rnn.pad_packed_sequence(output)\n",
    "#         else:\n",
    "#             output, _ = self.rnn(x)\n",
    "\n",
    "#         output = self.fc(output)\n",
    "\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rnn = RNNModelRNA(input_dim=14, hidden_dim=20, num_layers=3, output_dim=5).cuda()\n",
    "# rnn = LSTMModelRNA(input_dim=14, hidden_dim=20, num_layers=5, output_dim=5, dropout=0.2).cuda()\n",
    "# hidden = torch.zeros(1, 1, 3).to(\"cuda\")\n",
    "# optimizer = optim.Adam(rnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 100\n",
    "# batch_size_train = 480\n",
    "# batch_size_test = 480\n",
    "\n",
    "padding = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "train_batched = []\n",
    "prev = 0\n",
    "for i in range(batch_size_train, len(training) + 1, batch_size_train):\n",
    "    batch = training[prev:i]\n",
    "    batch = sorted(batch, key=lambda r: len(r[0]), reverse=True)\n",
    "    lengths_batch = list(map(lambda r: len(r[0]), batch))\n",
    "    max_length = max(lengths_batch)\n",
    "    batch_padded = list(map(lambda s: pad(s[0], max_length, padding=padding), batch))\n",
    "    batch_targets = list(map(lambda r: r[1], batch))\n",
    "    train_batched.append(((batch_padded, lengths_batch), batch_targets))\n",
    "    prev = i\n",
    "\n",
    "    \n",
    "evaluation_batched = []\n",
    "prev = 0\n",
    "for i in range(batch_size_test, len(evaluation) + 1, batch_size_test):\n",
    "    batch = evaluation[prev:i]\n",
    "    batch = sorted(batch, key=lambda r: len(r[0]), reverse=True)\n",
    "    lengths_batch = list(map(lambda r: len(r[0]), batch))\n",
    "    max_length = max(lengths_batch)\n",
    "    batch_padded = list(map(lambda s: pad(s[0], max_length, padding=padding), batch))\n",
    "    batch_targets = list(map(lambda r: r[1], batch))\n",
    "    evaluation_batched.append(((batch_padded, lengths_batch), batch_targets))\n",
    "    prev = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_batched), len(evaluation_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 318)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_batched[0][0][0]), len(evaluation_batched[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_tensor(data_batch, lengths, batchsize):\n",
    "    batch_tensors = list(map(lambda t: torch.tensor(t, dtype=torch.float32), data_batch))\n",
    "    return torch.stack(batch_tensors, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_batches = []\n",
    "evaluation_set_batches = []\n",
    "for ((data_batch, batch_lengths), target) in train_batched:\n",
    "    training_set_batches.append(((batch_to_tensor(data_batch, batch_lengths, batch_size_train), batch_lengths), target))\n",
    "\n",
    "for ((data_batch, batch_lengths), target) in evaluation_batched:\n",
    "    evaluation_set_batches.append(((batch_to_tensor(data_batch, batch_lengths, batch_size_test), batch_lengths), target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_batch = training_set_batches[0]\n",
    "((data_batch, lengths_batch), target) = training_set_batch\n",
    "# output = rnn(data_batch, lengths=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelRNA(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=None, hidden_dim=None, num_layers=None, output_dim=None):\n",
    "        super(RNNModelRNA, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(\"cuda\")\n",
    "    \n",
    "    def forward(self, x, lengths=None):\n",
    "        batch_size = x.size(1)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        if lengths is not None:\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, lengths)\n",
    "            output, hidden = self.rnn(x, hidden)\n",
    "            output, lengths_unpacked = nn.utils.rnn.pad_packed_sequence(output)\n",
    "        else:\n",
    "            output, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    \n",
    "class LSTMModelRNA(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=None, hidden_dim=None, num_layers=None, output_dim=None, dropout=0.1):\n",
    "        super(LSTMModelRNA, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(\"cuda\")\n",
    "    \n",
    "    def forward(self, x, lengths=None):\n",
    "        batch_size = x.size(1)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        if lengths is not None:\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, lengths)\n",
    "            output, _ = self.rnn(x)\n",
    "            output, lengths_unpacked = nn.utils.rnn.pad_packed_sequence(output)\n",
    "        else:\n",
    "            output, _ = self.rnn(x)\n",
    "\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn = RNNModelRNA(input_dim=14, hidden_dim=20, num_layers=3, output_dim=5).cuda()\n",
    "rnn = LSTMModelRNA(input_dim=14, hidden_dim=20, num_layers=9, output_dim=5, dropout=0.2).cuda()\n",
    "hidden = torch.zeros(1, 1, 3).to(\"cuda\")\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rnn(data_batch.to(\"cuda\"), lengths=lengths_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([107, 5])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:,0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    with torch.no_grad():\n",
    "        test_loss = torch.tensor(0.0).to(\"cuda\")\n",
    "        for ((data_batch, lengths_batch), target) in evaluation_set_batches:\n",
    "            output = rnn(data_batch.to(\"cuda\"), lengths=lengths_batch)\n",
    "            loss_sequence = torch.tensor(0.0).to(\"cuda\")\n",
    "            for i in range(batch_size_test):\n",
    "                loss_sequence += torch.nn.functional.mse_loss(output[:len(target[i]),i,:], torch.tensor(target[i]).to(\"cuda\"))\n",
    "            \n",
    "            test_loss += (loss_sequence / batch_size_test)\n",
    "        \n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_counter = range(num_epochs * len(training_set_batches))\n",
    "test_counter = range(num_epochs * len(training_set_batches))\n",
    "for n in range(num_epochs):\n",
    "    for ((data_batch, lengths_batch), target) in training_set_batches:\n",
    "        optimizer.zero_grad()\n",
    "        rnn.zero_grad()\n",
    "        output = rnn(data_batch.to(\"cuda\"), lengths=lengths_batch)\n",
    "        loss = torch.tensor(0.0).to(\"cuda\")\n",
    "        sequence_loss = torch.tensor(0.0).to(\"cuda\")\n",
    "        for i in range(batch_size_train):\n",
    "            sequence_loss += torch.nn.functional.mse_loss(output[:len(target[i]),i,:], torch.tensor(target[i]).to(\"cuda\"))\n",
    "        loss += (sequence_loss / batch_size_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_losses.append(torch.sqrt(loss).item())\n",
    "        test_loss = test()\n",
    "        test_losses.append(torch.sqrt(test_loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_counter, train_losses, color=\"b\", label=\"Train Loss\")\n",
    "ax.scatter(test_counter, test_losses, color=\"red\", label=\"Test Loss\")\n",
    "leg = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses[-1], test_losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id_00073f8be'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(testing_sequence_id_to_dataset_map.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = testing_sequence_id_to_dataset_map[\"id_00073f8be\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModelRNA(\n",
       "  (rnn): LSTM(14, 20, num_layers=9, dropout=0.2, bidirectional=True)\n",
       "  (fc): Linear(in_features=40, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.tensor(input_seq, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = input_seq.view(107, 1, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_seq = rnn(input_seq.to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([107, 1, 5])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seq.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7153, 0.7475, 2.1112, 0.6229, 0.7481], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seq[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_header = 'id_seqpos,reactivity,deg_Mg_pH10,deg_pH10,deg_Mg_50C,deg_50C'\n",
    "output_lines = [output_header]\n",
    "for sequence_id, test_seq in testing_sequence_id_to_dataset_map.items():\n",
    "    test_seq = torch.tensor(test_seq, dtype=torch.float32)\n",
    "    test_seq = test_seq.view(test_seq.size(0), 1, test_seq.size(1))\n",
    "    output = rnn(test_seq.to(\"cuda\"))\n",
    "    output = output.to(\"cpu\")\n",
    "    seq_length = output.size(0)\n",
    "    for i in range(seq_length):\n",
    "        seq_id_pos = f\"{sequence_id}_{i}\"\n",
    "        seq_id_pos_entry = list(map(lambda num: str(num), output[i,0,:].tolist()))\n",
    "        submission_line_entries = [seq_id_pos] + seq_id_pos_entry\n",
    "        submission_line = \",\".join(submission_line_entries)\n",
    "        output_lines.append(submission_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = \"\\n\".join(output_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id_00073f8be_0,0.7152865529060364,0.7475160360336304,2.1112401485443115,0.6229220628738403,0.7481404542922974'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lines[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file = open(\"datasets/stanford-covid-vaccine/submission-lstm.csv\", \"w\")\n",
    "submission_file.write(submission_data)\n",
    "submission_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
