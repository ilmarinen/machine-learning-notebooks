{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary(n):\n",
    "    binary_representation = []\n",
    "    while n > 0:\n",
    "        binary_representation.append((n & 1))\n",
    "        n = n >> 1\n",
    "\n",
    "    return binary_representation\n",
    "\n",
    "\n",
    "def get_decimal(binary_representation):\n",
    "    result = 0\n",
    "    k = 0\n",
    "    while len(binary_representation) > 0:\n",
    "        digit = binary_representation.pop()\n",
    "        result = result + digit * (2 ** k)\n",
    "        k += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "def pad(seq, length, padding=None):\n",
    "    if padding == None:\n",
    "        padding = 0\n",
    "    z = [padding] * (length - len(seq))\n",
    "    seq = seq + z\n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, 2)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        output = self.fc(output)\n",
    "        # output = F.softmax(output, dim=2)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class RNNModelEfficient(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(RNNModelEfficient, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, 3)\n",
    "    \n",
    "    def forward(self, x, lengths, hidden):\n",
    "        sequence_length = x.size(0)\n",
    "        batch_size = x.size(1)\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths)\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        output, lengths_unpacked = nn.utils.rnn.pad_packed_sequence(output)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class RNNModelFinal(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(RNNModelFinal, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, 3)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(\"cuda\")\n",
    "    \n",
    "    def forward(self, x, lengths=None):\n",
    "        batch_size = x.size(1)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        if lengths is not None:\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, lengths)\n",
    "            output, hidden = self.rnn(x, hidden)\n",
    "            output, lengths_unpacked = nn.utils.rnn.pad_packed_sequence(output)\n",
    "        else:\n",
    "            output, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNNModelFinal(2, 3, 1).cuda()\n",
    "hidden = torch.zeros(1, 1, 3).to(\"cuda\")\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i in range(500):\n",
    "    a = random.randrange(10000)\n",
    "    for j in range(500):\n",
    "        b = random.randrange(1000000)\n",
    "        c = a + b\n",
    "        x = get_binary(a)\n",
    "        y = get_binary(b)\n",
    "        z = get_binary(c) + [2]\n",
    "        max_length = max(len(x), len(y), len(z))\n",
    "        x = pad(x, max_length)\n",
    "        y = pad(y, max_length)\n",
    "        dataset.append((list(zip(x, y)), z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "batched_dataset = []\n",
    "prev = 0\n",
    "for i in range(batch_size, len(dataset), batch_size):\n",
    "    batch = dataset[prev:i]\n",
    "    batch = sorted(batch, key=lambda r: len(r[0]), reverse=True)\n",
    "    lengths_batch = list(map(lambda r: len(r[0]), batch))\n",
    "    max_length = max(lengths_batch)\n",
    "    batch_padded = list(map(lambda s: pad(s[0], max_length, padding=(0, 0)), batch))\n",
    "    batch_targets = list(map(lambda r: r[1], batch))\n",
    "    batched_dataset.append(((batch_padded, lengths_batch), batch_targets))\n",
    "    prev = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = batched_dataset[:int(0.8 * len(batched_dataset))]\n",
    "testing_set = batched_dataset[int(0.8 * len(batched_dataset)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_tensor(data_batch, lengths, batchsize):\n",
    "    batch_tensors = list(map(lambda t: torch.tensor(t, dtype=torch.float32), data_batch))\n",
    "    return torch.stack(batch_tensors, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_batches = []\n",
    "testing_set_batches = []\n",
    "for ((data_batch, batch_lengths), target) in training_set:\n",
    "    training_set_batches.append(((batch_to_tensor(data_batch, batch_lengths, batch_size), batch_lengths), target))\n",
    "\n",
    "for ((data_batch, batch_lengths), target) in testing_set:\n",
    "    testing_set_batches.append(((batch_to_tensor(data_batch, batch_lengths, batch_size), batch_lengths), target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(2):\n",
    "    for ((data_batch, lengths_batch), target) in training_set_batches:\n",
    "        hidden = torch.zeros(1, batch_size, 3).to(\"cuda\")\n",
    "        rnn.zero_grad()\n",
    "        output = rnn(data_batch.to(\"cuda\"), lengths=lengths_batch)\n",
    "        loss = 0\n",
    "        for i in range(batch_size):\n",
    "            loss += torch.nn.functional.cross_entropy(output[:lengths_batch[i],i,:], torch.tensor(target[i]).to(\"cuda\"))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id = 103\n",
    "test_b, lengths_b = testing_set_batches[batch_id][0]\n",
    "target_b = testing_set_batches[batch_id][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 100, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rnn(test_b.to(\"cuda\"), lengths=lengths_b)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.argmax(out, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 2, 1, 0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(target_b[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.stack([torch.tensor([1, 1, 0, 0, 0, 0, 0], dtype=torch.float32), torch.tensor([1, 0, 1, 0, 0, 0, 0], dtype=torch.float32)], dim=0).view(7, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.]],\n",
       "\n",
       "        [[0., 0.]],\n",
       "\n",
       "        [[0., 0.]],\n",
       "\n",
       "        [[0., 1.]],\n",
       "\n",
       "        [[0., 1.]],\n",
       "\n",
       "        [[0., 0.]],\n",
       "\n",
       "        [[0., 0.]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo = rnn(x.to(\"cuda\"), lengths=[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooo = torch.argmax(oo, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 1, 2, 0], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ooo[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
